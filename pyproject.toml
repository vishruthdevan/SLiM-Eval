[project]
name = "slim-eval"
version = "0.1.3"
description = "Complete Small Language Model Evaluation Framework - Tracking Latency, Memory, Energy, and Accuracy"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    # Data Processing & Analysis (required for analyze command)
    "pandas==2.3.3",
    "numpy==2.2.6",

    # Visualization (required for analyze command)
    "matplotlib==3.10.7",
    "seaborn==0.13.2",

    # CLI
    "typer==0.20.0",
]

[project.optional-dependencies]
all = [
    # Core PyTorch (required for model evaluation)
    "torch==2.8.0",
    "torchvision==0.23.0",
    "torchaudio==2.8.0",

    # LLM Inference & Quantization
    "vllm==0.11.0",
    "llmcompressor==0.7.1",
    "transformers==4.55.2",
    "accelerate==1.10.0",
    "tokenizers==0.21.4",

    # Evaluation & Benchmarking
    "lm-eval==0.4.9.2",

    # GPU Monitoring
    "nvidia-ml-py==12.575.51",
    "pynvml==12.0.0",

    # Progress & Utilities
    "tqdm==4.67.1",
    "psutil==7.1.3",    
    "wandb==0.18.5",
]
dev = [
    "jupyter==1.0.0",
    "ipykernel==6.25.0",
    "ipywidgets==8.1.0",
]

[project.scripts]
slim-eval = "slim_eval.cli:main"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["slim_eval", "slim_eval.benchmarks"]
