{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d28278",
   "metadata": {},
   "source": [
    "# SLiM-Eval: Complete Small Language Model Evaluation Framework\n",
    "## Tracking: Latency, Memory, Energy, and Accuracy\n",
    "\n",
    "This notebook contains:\n",
    "1. Environment setup and verification\n",
    "2. Model quantization using llm-compressor\n",
    "3. **Latency and memory** benchmarking using vLLM\n",
    "4. **Energy consumption** tracking using CodeCarbon/powermetrics\n",
    "5. **Accuracy evaluation** using lm-evaluation-harness\n",
    "6. Results analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081c583",
   "metadata": {},
   "source": [
    "## Cell 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if packages are not installed\n",
    "# !pip install torch>=2.9.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install vllm>=0.5.0\n",
    "# !pip install llm-compressor\n",
    "# !pip install transformers>=4.40.0\n",
    "# !pip install accelerate\n",
    "# !pip install pandas numpy tqdm\n",
    "# !pip install matplotlib seaborn\n",
    "# !pip install lm-eval>=0.4.0  # For accuracy evaluation\n",
    "# !pip install codecarbon  # For energy tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a070da",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2476b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import codecarbon for energy tracking\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Import lm-eval for accuracy\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.models.vllm_causallms import VLLM as VLLM_LM\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fef34",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration - Edit This Cell to Customize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EDITABLE MODEL LIST - Add or remove models as needed\n",
    "# ============================================================================\n",
    "MODELS = [\n",
    "    # \"microsoft/Phi-3-mini-4k-instruct\",  # Phi-3 3.8B\n",
    "    # \"microsoft/Phi-3.5-mini-instruct\",  # Phi-3.5 3.8B\n",
    "    # \"google/gemma-2-2b-it\",  # Gemma 2 2B\n",
    "    \"meta-llama/Llama-3.2-3B\",  # Llama 3.2 3B\n",
    "    # \"Qwen/Qwen2.5-3B-Instruct\",  # Qwen2.5 3B\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.3\",  # Mistral 7B\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARK CONFIGURATION\n",
    "# ============================================================================\n",
    "# LATENCY & MEMORY BENCHMARKING\n",
    "NUM_RUNS = 1000  # Number of latency benchmark runs\n",
    "NUM_WARMUP = 10  # Warmup runs to exclude from metrics\n",
    "MAX_NEW_TOKENS = 32  # Tokens to generate per inference\n",
    "BATCH_SIZE = 32  # Batch size for vLLM (higher = faster)\n",
    "PROMPT = \"Explain one interesting fact about large language models.\"\n",
    "\n",
    "# ACCURACY EVALUATION\n",
    "ACCURACY_TASKS = [\n",
    "    \"mmlu\",  # MMLU (knowledge)\n",
    "    \"gsm8k\",  # GSM8K (math reasoning)\n",
    "    \"hellaswag\",  # HellaSwag (commonsense reasoning)\n",
    "    # \"humaneval\",       # HumanEval (code) - requires unsafe code execution\n",
    "]\n",
    "NUM_FEW_SHOT = 5  # Few-shot examples for accuracy tasks\n",
    "ACCURACY_LIMIT = None  # Set to small number (e.g., 100) for quick testing\n",
    "\n",
    "# ENERGY TRACKING\n",
    "ENABLE_ENERGY_TRACKING = True  # Enable/disable energy monitoring\n",
    "ENERGY_SAMPLE_RUNS = 100  # Number of runs for energy measurement\n",
    "\n",
    "# ============================================================================\n",
    "# QUANTIZATION PRECISIONS TO TEST\n",
    "# ============================================================================\n",
    "PRECISIONS = [\"fp16\", \"int8\", \"int4\"]  # Can add \"gptq\", \"awq\" later\n",
    "\n",
    "# ============================================================================\n",
    "# OUTPUT CONFIGURATION\n",
    "# ============================================================================\n",
    "OUTPUT_DIR = Path(\"slim_eval_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_CSV = OUTPUT_DIR / \"complete_results.csv\"\n",
    "QUANTIZED_MODELS_DIR = Path(\"quantized_models\")\n",
    "QUANTIZED_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Models: {len(MODELS)}\")\n",
    "print(f\"  Precisions: {PRECISIONS}\")\n",
    "print(f\"  Latency runs: {NUM_RUNS}\")\n",
    "print(f\"  Accuracy tasks: {ACCURACY_TASKS}\")\n",
    "print(f\"  Energy tracking: {ENABLE_ENERGY_TRACKING}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02500a15",
   "metadata": {},
   "source": [
    "## Cell 4: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache():\n",
    "    \"\"\"Clear GPU cache and run garbage collection.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def get_gpu_memory_mb() -> float:\n",
    "    \"\"\"Get current GPU memory usage in MB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024**2)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def get_peak_gpu_memory_mb() -> float:\n",
    "    \"\"\"Get peak GPU memory usage in MB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def print_gpu_memory_status():\n",
    "    \"\"\"Print current GPU memory status.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(\n",
    "            f\"GPU Memory: {allocated:.2f}GB allocated | {reserved:.2f}GB reserved | {total:.2f}GB total\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Utility functions loaded ✓\")\n",
    "print_gpu_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a169881",
   "metadata": {},
   "source": [
    "## Cell 5: Quantization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daffb985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier, QuantizationModifier\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "# Quantization configurations\n",
    "QUANTIZATION_CONFIGS = {\n",
    "    \"int8\": {\n",
    "        \"recipe\": [\n",
    "            SmoothQuantModifier(smoothing_strength=0.8),\n",
    "            GPTQModifier(targets=\"Linear\", scheme=\"W8A8\", ignore=[\"lm_head\"]),\n",
    "        ],\n",
    "        \"method\": \"gptq_smoothquant\",\n",
    "    },\n",
    "    \"int4\": {\n",
    "        \"recipe\": [\n",
    "            SmoothQuantModifier(smoothing_strength=0.8),\n",
    "            GPTQModifier(targets=\"Linear\", scheme=\"W4A16\", ignore=[\"lm_head\"]),\n",
    "        ],\n",
    "        \"method\": \"gptq_smoothquant\",\n",
    "    },\n",
    "    \"gptq\": {\n",
    "        \"recipe\": GPTQModifier(\n",
    "            targets=\"Linear\",\n",
    "            scheme=\"W4A16\",\n",
    "            ignore=[\"lm_head\"],\n",
    "        ),\n",
    "        \"method\": \"gptq_only\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Calibration dataset configuration\n",
    "CALIBRATION_DATASET = \"HuggingFaceH4/ultrachat_200k\"\n",
    "CALIBRATION_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "\n",
    "print(\"Quantization configurations loaded ✓\")\n",
    "print(\"  - INT8: W8A8 with SmoothQuant\")\n",
    "print(\"  - INT4: W4A16 with SmoothQuant + GPTQ\")\n",
    "print(\"  - GPTQ: W4A16 GPTQ only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cffb3",
   "metadata": {},
   "source": [
    "## Cell 6: Quantization Function (Skip if using pre-quantized models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model_name: str, precision: str, output_dir: Path):\n",
    "    \"\"\"Quantize a model using llm-compressor with SmoothQuant + GPTQ.\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Quantizing {model_name} to {precision.upper()}\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    if output_dir.exists() and (output_dir / \"config.json\").exists():\n",
    "        print(f\"✓ Already quantized, skipping...\")\n",
    "        return\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        if precision not in QUANTIZATION_CONFIGS:\n",
    "            print(f\"✗ Unsupported precision: {precision}\")\n",
    "            return\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        print(f\"Loading model and tokenizer...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Load and preprocess calibration dataset\n",
    "        print(f\"Loading calibration dataset: {CALIBRATION_DATASET}\")\n",
    "        ds = load_dataset(\n",
    "            CALIBRATION_DATASET,\n",
    "            split=f\"{CALIBRATION_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\",\n",
    "        )\n",
    "        ds = ds.shuffle(seed=42)\n",
    "\n",
    "        # Preprocess with chat template (with fallback for models without chat template)\n",
    "        def preprocess(example):\n",
    "            try:\n",
    "                # Try to use chat template if available\n",
    "                return {\n",
    "                    \"text\": tokenizer.apply_chat_template(\n",
    "                        example[\"messages\"],\n",
    "                        tokenize=False,\n",
    "                    )\n",
    "                }\n",
    "            except Exception:\n",
    "                # Fallback: concatenate messages manually\n",
    "                messages = example[\"messages\"]\n",
    "                text_parts = []\n",
    "                for msg in messages:\n",
    "                    role = msg.get(\"role\", \"\")\n",
    "                    content = msg.get(\"content\", \"\")\n",
    "                    text_parts.append(f\"{role}: {content}\")\n",
    "                return {\"text\": \"\\n\".join(text_parts)}\n",
    "\n",
    "        ds = ds.map(preprocess)\n",
    "\n",
    "        # Tokenize\n",
    "        def tokenize(sample):\n",
    "            return tokenizer(\n",
    "                sample[\"text\"],\n",
    "                padding=False,\n",
    "                max_length=MAX_SEQUENCE_LENGTH,\n",
    "                truncation=True,\n",
    "                add_special_tokens=False,\n",
    "            )\n",
    "\n",
    "        ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "        # Get quantization recipe\n",
    "        quant_config = QUANTIZATION_CONFIGS[precision]\n",
    "        recipe = quant_config[\"recipe\"]\n",
    "\n",
    "        print(f\"Applying quantization recipe: {quant_config['method']}\")\n",
    "\n",
    "        # Apply quantization\n",
    "        oneshot(\n",
    "            model=model,\n",
    "            dataset=ds,\n",
    "            recipe=recipe,\n",
    "            max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "            num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "        )\n",
    "\n",
    "        # Verify with sample generation\n",
    "        print(\"\\n========== SAMPLE GENERATION ==============\")\n",
    "        dispatch_for_generation(model)\n",
    "        input_ids = tokenizer(\n",
    "            \"Hello my name is\", return_tensors=\"pt\"\n",
    "        ).input_ids.to(model.device)\n",
    "        output = model.generate(input_ids, max_new_tokens=50)\n",
    "        print(tokenizer.decode(output[0]))\n",
    "        print(\"==========================================\\n\")\n",
    "\n",
    "        # Save quantized model\n",
    "        print(f\"Saving to {output_dir}...\")\n",
    "        model.save_pretrained(output_dir, save_compressed=True)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        print(f\"✓ Quantization complete: {output_dir}\")\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        clear_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Quantization failed: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "print(\"Quantization function loaded ✓\")\n",
    "print(\"  - Uses SmoothQuant + GPTQ for improved quality\")\n",
    "print(\"  - Includes generation verification\")\n",
    "print(\"  - Auto-handles models with/without chat templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c58fae",
   "metadata": {},
   "source": [
    "## Cell 7: vLLM Model Setup Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vllm_model(\n",
    "    model_name: str, precision: str, use_quantized_dir: bool = True\n",
    ") -> Optional[LLM]:\n",
    "    \"\"\"Setup vLLM model with specified precision.\"\"\"\n",
    "    clear_cache()\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Loading {model_name} in {precision.upper()} precision...\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        if precision == \"fp16\":\n",
    "            model_path = model_name\n",
    "            dtype = \"float16\"\n",
    "            quantization = None\n",
    "        else:\n",
    "            model_short_name = model_name.split(\"/\")[-1]\n",
    "            quantized_path = QUANTIZED_MODELS_DIR / f\"{model_short_name}_{precision}\"\n",
    "\n",
    "            if use_quantized_dir and quantized_path.exists():\n",
    "                # Pre-quantized model exists\n",
    "                model_path = str(quantized_path)\n",
    "                dtype = \"auto\"\n",
    "                quantization = None  # Model is already quantized, vLLM auto-detects\n",
    "                print(f\"Using pre-quantized model from: {quantized_path}\")\n",
    "                print(f\"  Method: {QUANTIZATION_CONFIGS[precision]['method']}\")\n",
    "            elif use_quantized_dir:\n",
    "                # Pre-quantized model doesn't exist, create it\n",
    "                print(f\"Pre-quantized model not found at: {quantized_path}\")\n",
    "                print(f\"Running quantization with llm-compressor...\")\n",
    "                quantize_model(model_name, precision, quantized_path)\n",
    "                \n",
    "                # Now use the quantized model\n",
    "                if quantized_path.exists():\n",
    "                    model_path = str(quantized_path)\n",
    "                    dtype = \"auto\"\n",
    "                    quantization = None\n",
    "                    print(f\"Using newly quantized model from: {quantized_path}\")\n",
    "                else:\n",
    "                    # Quantization failed, fall back to on-the-fly\n",
    "                    print(f\"⚠️  Quantization failed, falling back to on-the-fly quantization\")\n",
    "                    model_path = model_name\n",
    "                    dtype = \"auto\"\n",
    "                    if precision == \"int8\":\n",
    "                        quantization = \"int8\"\n",
    "                    elif precision == \"int4\":\n",
    "                        quantization = \"int4\"\n",
    "                    elif precision == \"gptq\":\n",
    "                        quantization = \"gptq\"\n",
    "                    elif precision == \"awq\":\n",
    "                        quantization = \"awq\"\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown precision: {precision}\")\n",
    "            else:\n",
    "                # On-the-fly quantization (use_quantized_dir=False)\n",
    "                model_path = model_name\n",
    "                dtype = \"auto\"\n",
    "                print(f\"⚠️  Using base model with on-the-fly quantization: {precision}\")\n",
    "                print(f\"   For best quality, pre-quantize with llm-compressor first\")\n",
    "                \n",
    "                # Set vLLM quantization method\n",
    "                if precision == \"int8\":\n",
    "                    quantization = \"int8\"\n",
    "                elif precision == \"int4\":\n",
    "                    quantization = \"int4\"\n",
    "                elif precision == \"gptq\":\n",
    "                    quantization = \"gptq\"\n",
    "                elif precision == \"awq\":\n",
    "                    quantization = \"awq\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown precision: {precision}\")\n",
    "\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            dtype=dtype,\n",
    "            quantization=quantization,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=2048,\n",
    "            tensor_parallel_size=1,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        print(f\"✓ Model loaded successfully\")\n",
    "        print_gpu_memory_status()\n",
    "        return llm\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load {model_name} in {precision}: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Model setup function loaded ✓\")\n",
    "print(\"  - Auto-detects pre-quantized models from llm-compressor\")\n",
    "print(\"  - Automatically quantizes models if not found\")\n",
    "print(\"  - Falls back to vLLM on-the-fly quantization if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde46cda",
   "metadata": {},
   "source": [
    "## Cell 8: Latency & Memory Benchmarking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency_memory(\n",
    "    llm: LLM,\n",
    "    model_name: str,\n",
    "    precision: str,\n",
    "    num_runs: int = NUM_RUNS,\n",
    "    num_warmup: int = NUM_WARMUP,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    ") -> Dict:\n",
    "    \"\"\"Benchmark vLLM model for latency and memory.\"\"\"\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "    latencies = []\n",
    "    peak_memories = []\n",
    "    avg_memories = []\n",
    "    tokens_generated = []\n",
    "\n",
    "    total_iterations = (num_runs + num_warmup + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"LATENCY & MEMORY BENCHMARK\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Warmup: {num_warmup} | Benchmark: {num_runs} | Batch: {batch_size}\")\n",
    "\n",
    "    iteration_count = 0\n",
    "    pbar = tqdm(total=num_runs + num_warmup, desc=\"Latency/Memory\")\n",
    "\n",
    "    for batch_idx in range(total_iterations):\n",
    "        current_batch_size = min(batch_size, num_runs + num_warmup - iteration_count)\n",
    "        prompts = [PROMPT] * current_batch_size\n",
    "\n",
    "        clear_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        start_time = time.time()\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "\n",
    "        batch_latency = end_time - start_time\n",
    "        peak_mem = get_peak_gpu_memory_mb()\n",
    "        avg_mem = get_gpu_memory_mb()\n",
    "        per_request_latency = batch_latency / current_batch_size\n",
    "        batch_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n",
    "\n",
    "        for i in range(current_batch_size):\n",
    "            if iteration_count >= num_warmup:\n",
    "                latencies.append(per_request_latency)\n",
    "                peak_memories.append(peak_mem)\n",
    "                avg_memories.append(avg_mem)\n",
    "                tokens_generated.append(batch_tokens / current_batch_size)\n",
    "\n",
    "            iteration_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            if iteration_count >= num_runs + num_warmup:\n",
    "                break\n",
    "\n",
    "        if iteration_count >= num_runs + num_warmup:\n",
    "            break\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    lat = np.array(latencies)\n",
    "    pm = np.array(peak_memories)\n",
    "    am = np.array(avg_memories)\n",
    "    tg = np.array(tokens_generated)\n",
    "\n",
    "    results = {\n",
    "        \"mean_latency_s\": lat.mean(),\n",
    "        \"median_latency_s\": np.median(lat),\n",
    "        \"p95_latency_s\": np.percentile(lat, 95),\n",
    "        \"p99_latency_s\": np.percentile(lat, 99),\n",
    "        \"std_latency_s\": lat.std(),\n",
    "        \"mean_peak_mem_mb\": pm.mean(),\n",
    "        \"mean_avg_mem_mb\": am.mean(),\n",
    "        \"tokens_per_second\": tg.mean() / lat.mean(),\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"✓ Latency: {results['mean_latency_s']:.4f}s | Memory: {results['mean_peak_mem_mb']:.2f}MB\"\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Latency/Memory benchmark function loaded ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe564c2",
   "metadata": {},
   "source": [
    "## Cell 9: Energy Tracking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f70329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_energy(\n",
    "    llm: LLM, model_name: str, precision: str, num_samples: int = ENERGY_SAMPLE_RUNS\n",
    ") -> Dict:\n",
    "    \"\"\"Measure energy consumption using CodeCarbon.\"\"\"\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"ENERGY CONSUMPTION BENCHMARK\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Running {num_samples} inference samples with energy tracking...\")\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"Write a Python function to calculate fibonacci numbers.\",\n",
    "        \"What are the main causes of climate change?\",\n",
    "        \"Solve: If x + 5 = 12, what is x?\",\n",
    "        \"Describe the process of photosynthesis.\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Initialize energy tracker\n",
    "        tracker = EmissionsTracker(\n",
    "            project_name=f\"slim_eval_{model_name.split('/')[-1]}_{precision}\",\n",
    "            output_dir=str(OUTPUT_DIR / \"energy_logs\"),\n",
    "            log_level=\"warning\",\n",
    "            save_to_file=True,\n",
    "        )\n",
    "\n",
    "        # Start tracking\n",
    "        tracker.start()\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Run inference samples\n",
    "        for i in tqdm(range(num_samples), desc=\"Energy tracking\"):\n",
    "            prompt = test_prompts[i % len(test_prompts)]\n",
    "            prompts_batch = [prompt]\n",
    "            outputs = llm.generate(prompts_batch, sampling_params)\n",
    "\n",
    "        # Stop tracking\n",
    "        end_time = time.time()\n",
    "        emissions = tracker.stop()\n",
    "\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        results = {\n",
    "            \"energy_kwh\": emissions,  # kWh\n",
    "            \"energy_joules\": emissions * 3600000,  # Convert kWh to Joules\n",
    "            \"duration_seconds\": duration,\n",
    "            \"avg_power_watts\": (emissions * 3600000 / duration) if duration > 0 else 0,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"energy_per_query_j\": (emissions * 3600000 / num_samples)\n",
    "            if num_samples > 0\n",
    "            else 0,\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"✓ Energy: {results['energy_kwh'] * 1000:.4f} Wh | Avg Power: {results['avg_power_watts']:.2f}W\"\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Energy tracking failed: {e}\")\n",
    "        # Return fallback results with timing only\n",
    "        return {\n",
    "            \"energy_kwh\": 0,\n",
    "            \"energy_joules\": 0,\n",
    "            \"duration_seconds\": 0,\n",
    "            \"avg_power_watts\": 0,\n",
    "            \"num_samples\": 0,\n",
    "            \"energy_per_query_j\": 0,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Energy benchmark function loaded ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316008f9",
   "metadata": {},
   "source": [
    "## Cell 10: Accuracy Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e79e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_accuracy(\n",
    "    model_name: str,\n",
    "    precision: str,\n",
    "    tasks: List[str] = ACCURACY_TASKS,\n",
    "    num_fewshot: int = NUM_FEW_SHOT,\n",
    "    limit: Optional[int] = ACCURACY_LIMIT,\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate model accuracy using lm-evaluation-harness with vLLM backend.\"\"\"\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"ACCURACY EVALUATION\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Tasks: {', '.join(tasks)}\")\n",
    "    print(f\"Few-shot: {num_fewshot}\")\n",
    "    if limit:\n",
    "        print(f\"Limit: {limit} examples per task\")\n",
    "\n",
    "    try:\n",
    "        # Determine model path and arguments\n",
    "        if precision == \"fp16\":\n",
    "            model_path = model_name\n",
    "            model_args = f\"pretrained={model_path},dtype=float16,gpu_memory_utilization=0.9,trust_remote_code=True,max_model_len=2048,tensor_parallel_size=1\"\n",
    "        else:\n",
    "            model_short_name = model_name.split(\"/\")[-1]\n",
    "            quantized_path = QUANTIZED_MODELS_DIR / f\"{model_short_name}_{precision}\"\n",
    "            \n",
    "            # Use quantized model if it exists, otherwise use base model with quantization flag\n",
    "            if quantized_path.exists():\n",
    "                model_path = str(quantized_path)\n",
    "                print(f\"Using pre-quantized model: {model_path}\")\n",
    "            else:\n",
    "                model_path = model_name\n",
    "                print(f\"Using base model with on-the-fly quantization: {precision}\")\n",
    "            \n",
    "            # Build model arguments for vLLM\n",
    "            model_args = f\"pretrained={model_path},dtype=auto,gpu_memory_utilization=0.9,trust_remote_code=True,max_model_len=2048,tensor_parallel_size=1\"\n",
    "            \n",
    "            # Add quantization parameter if using base model\n",
    "            if not quantized_path.exists():\n",
    "                if precision in [\"int8\", \"int4\", \"gptq\", \"awq\"]:\n",
    "                    model_args += f\",quantization={precision}\"\n",
    "\n",
    "        # Run evaluation using lm-eval CLI-style interface\n",
    "        results = evaluator.simple_evaluate(\n",
    "            model=\"vllm\",\n",
    "            model_args=model_args,\n",
    "            tasks=tasks,\n",
    "            num_fewshot=num_fewshot,\n",
    "            batch_size=\"auto\",\n",
    "            limit=limit,\n",
    "            log_samples=False,\n",
    "        )\n",
    "\n",
    "        # Extract metrics\n",
    "        accuracy_results = {}\n",
    "        for task_name, task_results in results[\"results\"].items():\n",
    "            # Find accuracy metric (different tasks use different metric names)\n",
    "            if \"acc\" in task_results:\n",
    "                accuracy_results[f\"{task_name}_accuracy\"] = task_results[\"acc\"]\n",
    "            elif \"acc_norm\" in task_results:\n",
    "                accuracy_results[f\"{task_name}_accuracy\"] = task_results[\"acc_norm\"]\n",
    "            elif \"exact_match\" in task_results:\n",
    "                accuracy_results[f\"{task_name}_accuracy\"] = task_results[\"exact_match\"]\n",
    "            elif \"pass@1\" in task_results:\n",
    "                accuracy_results[f\"{task_name}_accuracy\"] = task_results[\"pass@1\"]\n",
    "            else:\n",
    "                # Take the first metric that looks like accuracy\n",
    "                for key, value in task_results.items():\n",
    "                    if isinstance(value, (int, float)) and 0 <= value <= 1:\n",
    "                        accuracy_results[f\"{task_name}_accuracy\"] = value\n",
    "                        break\n",
    "\n",
    "        print(f\"✓ Accuracy results:\")\n",
    "        for task, acc in accuracy_results.items():\n",
    "            print(f\"  {task}: {acc:.4f} ({acc * 100:.2f}%)\")\n",
    "\n",
    "        return accuracy_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Accuracy evaluation failed: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return {f\"{task}_accuracy\": 0 for task in tasks}\n",
    "\n",
    "\n",
    "print(\"Accuracy benchmark function loaded ✓\")\n",
    "print(\"  - Uses lm-eval with vLLM backend\")\n",
    "print(\"  - Supports both pre-quantized and on-the-fly quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49323013",
   "metadata": {},
   "source": [
    "## Cell 11: Main Benchmarking Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_benchmark(model_name: str, precision: str) -> Dict:\n",
    "    \"\"\"Run complete benchmark: latency, memory, energy, and accuracy.\"\"\"\n",
    "\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"COMPLETE BENCHMARK: {model_name} ({precision.upper()})\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"precision\": precision,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    # Setup model\n",
    "    llm = setup_vllm_model(model_name, precision)\n",
    "    if llm is None:\n",
    "        print(f\"✗ Model setup failed, skipping...\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # 1. Latency & Memory\n",
    "        latency_memory_results = benchmark_latency_memory(llm, model_name, precision)\n",
    "        results.update(latency_memory_results)\n",
    "\n",
    "        # 2. Energy (if enabled)\n",
    "        if ENABLE_ENERGY_TRACKING:\n",
    "            energy_results = benchmark_energy(llm, model_name, precision)\n",
    "            results.update(energy_results)\n",
    "\n",
    "        # Clean up vLLM model before accuracy eval\n",
    "        del llm\n",
    "        clear_cache()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 3. Accuracy\n",
    "        accuracy_results = benchmark_accuracy(model_name, precision)\n",
    "        results.update(accuracy_results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Benchmark failed: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        clear_cache()\n",
    "\n",
    "\n",
    "print(\"Complete benchmark function loaded ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afb7bc",
   "metadata": {},
   "source": [
    "## Cell 12: Initialize Results CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all possible columns\n",
    "base_columns = [\n",
    "    \"model\",\n",
    "    \"precision\",\n",
    "    \"timestamp\",\n",
    "    \"mean_latency_s\",\n",
    "    \"median_latency_s\",\n",
    "    \"p95_latency_s\",\n",
    "    \"p99_latency_s\",\n",
    "    \"std_latency_s\",\n",
    "    \"mean_peak_mem_mb\",\n",
    "    \"mean_avg_mem_mb\",\n",
    "    \"tokens_per_second\",\n",
    "]\n",
    "\n",
    "energy_columns = [\n",
    "    \"energy_kwh\",\n",
    "    \"energy_joules\",\n",
    "    \"duration_seconds\",\n",
    "    \"avg_power_watts\",\n",
    "    \"energy_per_query_j\",\n",
    "]\n",
    "\n",
    "accuracy_columns = [f\"{task}_accuracy\" for task in ACCURACY_TASKS]\n",
    "\n",
    "all_columns = base_columns + energy_columns + accuracy_columns\n",
    "\n",
    "if not RESULTS_CSV.exists():\n",
    "    pd.DataFrame(columns=all_columns).to_csv(RESULTS_CSV, index=False)\n",
    "    print(f\"✓ Created results CSV: {RESULTS_CSV}\")\n",
    "else:\n",
    "    print(f\"✓ Results CSV exists: {RESULTS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf7088",
   "metadata": {},
   "source": [
    "## Cell 13: Run All Benchmarks\n",
    "**This cell runs everything. Will take several hours!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf18d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'#' * 70}\")\n",
    "print(\"SLiM-Eval: COMPLETE BENCHMARK SUITE\")\n",
    "print(f\"{'#' * 70}\\n\")\n",
    "print(f\"Models: {len(MODELS)}\")\n",
    "print(f\"Precisions: {PRECISIONS}\")\n",
    "print(f\"Total configs: {len(MODELS) * len(PRECISIONS)}\")\n",
    "print(f\"Metrics: Latency, Memory, Energy, Accuracy\")\n",
    "print(f\"Output: {RESULTS_CSV}\")\n",
    "print(f\"\\nEstimated time: ~{len(MODELS) * len(PRECISIONS) * 30} minutes\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    for precision in PRECISIONS:\n",
    "        config_id = f\"{model_name.split('/')[-1]}_{precision}\"\n",
    "\n",
    "        # Run complete benchmark\n",
    "        results = run_complete_benchmark(model_name, precision)\n",
    "\n",
    "        if results:\n",
    "            all_results.append(results)\n",
    "\n",
    "            # Save incrementally\n",
    "            pd.DataFrame([results]).to_csv(\n",
    "                RESULTS_CSV, mode=\"a\", header=False, index=False\n",
    "            )\n",
    "            print(f\"\\n✓ Results saved for {config_id}\")\n",
    "\n",
    "        # Cleanup between configs\n",
    "        clear_cache()\n",
    "        time.sleep(5)\n",
    "\n",
    "print(f\"\\n{'#' * 70}\")\n",
    "print(\"ALL BENCHMARKS COMPLETE!\")\n",
    "print(f\"{'#' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dd566",
   "metadata": {},
   "source": [
    "## Cell 14: Load and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete results\n",
    "results_df = pd.read_csv(RESULTS_CSV)\n",
    "\n",
    "print(f\"\\n{'#' * 70}\")\n",
    "print(\"COMPLETE RESULTS SUMMARY\")\n",
    "print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "# Display key metrics\n",
    "display_columns = [\n",
    "    \"model\",\n",
    "    \"precision\",\n",
    "    \"mean_latency_s\",\n",
    "    \"mean_peak_mem_mb\",\n",
    "    \"energy_kwh\",\n",
    "    \"avg_power_watts\",\n",
    "] + [col for col in results_df.columns if \"accuracy\" in col]\n",
    "\n",
    "print(results_df[display_columns].round(4).to_string(index=False))\n",
    "\n",
    "print(f\"\\n✓ Full results: {RESULTS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e04a6c",
   "metadata": {},
   "source": [
    "## Cell 15: Analysis - Efficiency vs Accuracy Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate efficiency metrics\n",
    "analysis_results = []\n",
    "\n",
    "for model in results_df[\"model\"].unique():\n",
    "    model_data = results_df[results_df[\"model\"] == model]\n",
    "    fp16_data = model_data[model_data[\"precision\"] == \"fp16\"]\n",
    "\n",
    "    if len(fp16_data) == 0:\n",
    "        continue\n",
    "\n",
    "    fp16_latency = fp16_data[\"mean_latency_s\"].values[0]\n",
    "    fp16_memory = fp16_data[\"mean_peak_mem_mb\"].values[0]\n",
    "    fp16_energy = (\n",
    "        fp16_data[\"energy_joules\"].values[0]\n",
    "        if \"energy_joules\" in fp16_data.columns\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    for _, row in model_data.iterrows():\n",
    "        if row[\"precision\"] != \"fp16\":\n",
    "            analysis_row = {\n",
    "                \"model\": model.split(\"/\")[-1],\n",
    "                \"precision\": row[\"precision\"],\n",
    "                \"speedup\": fp16_latency / row[\"mean_latency_s\"],\n",
    "                \"memory_reduction_pct\": (1 - row[\"mean_peak_mem_mb\"] / fp16_memory)\n",
    "                * 100,\n",
    "            }\n",
    "\n",
    "            # Energy reduction\n",
    "            if \"energy_joules\" in row and fp16_energy > 0:\n",
    "                analysis_row[\"energy_reduction_pct\"] = (\n",
    "                    1 - row[\"energy_joules\"] / fp16_energy\n",
    "                ) * 100\n",
    "\n",
    "            # Accuracy drop\n",
    "            for task in ACCURACY_TASKS:\n",
    "                acc_col = f\"{task}_accuracy\"\n",
    "                if acc_col in fp16_data.columns and acc_col in row:\n",
    "                    fp16_acc = fp16_data[acc_col].values[0]\n",
    "                    quant_acc = row[acc_col]\n",
    "                    if fp16_acc > 0:\n",
    "                        analysis_row[f\"{task}_acc_drop_pct\"] = (\n",
    "                            (fp16_acc - quant_acc) / fp16_acc\n",
    "                        ) * 100\n",
    "\n",
    "            analysis_results.append(analysis_row)\n",
    "\n",
    "if analysis_results:\n",
    "    analysis_df = pd.DataFrame(analysis_results)\n",
    "    print(\"\\n{'#'*70}\")\n",
    "    print(\"QUANTIZATION IMPACT ANALYSIS\")\n",
    "    print(f\"{'#' * 70}\\n\")\n",
    "    print(analysis_df.round(3).to_string(index=False))\n",
    "\n",
    "    analysis_path = OUTPUT_DIR / \"quantization_impact.csv\"\n",
    "    analysis_df.to_csv(analysis_path, index=False)\n",
    "    print(f\"\\n✓ Analysis saved: {analysis_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ceba8c",
   "metadata": {},
   "source": [
    "## Cell 16: Visualization - Pareto Frontiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Latency vs Memory\n",
    "ax1 = axes[0, 0]\n",
    "for precision in results_df[\"precision\"].unique():\n",
    "    data = results_df[results_df[\"precision\"] == precision]\n",
    "    ax1.scatter(\n",
    "        data[\"mean_latency_s\"],\n",
    "        data[\"mean_peak_mem_mb\"],\n",
    "        label=precision.upper(),\n",
    "        s=150,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "ax1.set_xlabel(\"Latency (seconds)\")\n",
    "ax1.set_ylabel(\"Peak Memory (MB)\")\n",
    "ax1.set_title(\"Latency vs Memory\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Energy vs Accuracy (if available)\n",
    "if \"energy_kwh\" in results_df.columns and any(\n",
    "    \"accuracy\" in col for col in results_df.columns\n",
    "):\n",
    "    ax2 = axes[0, 1]\n",
    "    acc_col = [col for col in results_df.columns if \"accuracy\" in col][0]\n",
    "    for precision in results_df[\"precision\"].unique():\n",
    "        data = results_df[results_df[\"precision\"] == precision]\n",
    "        ax2.scatter(\n",
    "            data[\"energy_kwh\"], data[acc_col], label=precision.upper(), s=150, alpha=0.7\n",
    "        )\n",
    "    ax2.set_xlabel(\"Energy (kWh)\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_title(\"Energy vs Accuracy Trade-off\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Throughput comparison\n",
    "ax3 = axes[1, 0]\n",
    "models_short = results_df[\"model\"].str.split(\"/\").str[-1]\n",
    "for precision in results_df[\"precision\"].unique():\n",
    "    data = results_df[results_df[\"precision\"] == precision]\n",
    "    indices = data.index\n",
    "    ax3.bar(\n",
    "        [models_short[i] for i in indices],\n",
    "        data[\"tokens_per_second\"],\n",
    "        label=precision.upper(),\n",
    "        alpha=0.7,\n",
    "    )\n",
    "ax3.set_xlabel(\"Model\")\n",
    "ax3.set_ylabel(\"Tokens/Second\")\n",
    "ax3.set_title(\"Throughput by Model and Precision\")\n",
    "ax3.legend()\n",
    "ax3.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Plot 4: Accuracy comparison across tasks\n",
    "ax4 = axes[1, 1]\n",
    "accuracy_cols = [col for col in results_df.columns if \"accuracy\" in col]\n",
    "if accuracy_cols:\n",
    "    # Average accuracy across all tasks\n",
    "    results_df[\"avg_accuracy\"] = results_df[accuracy_cols].mean(axis=1)\n",
    "    for precision in results_df[\"precision\"].unique():\n",
    "        data = results_df[results_df[\"precision\"] == precision]\n",
    "        indices = data.index\n",
    "        ax4.bar(\n",
    "            [models_short[i] for i in indices],\n",
    "            data[\"avg_accuracy\"],\n",
    "            label=precision.upper(),\n",
    "            alpha=0.7,\n",
    "        )\n",
    "    ax4.set_xlabel(\"Model\")\n",
    "    ax4.set_ylabel(\"Average Accuracy\")\n",
    "    ax4.set_title(\"Average Accuracy Across Tasks\")\n",
    "    ax4.legend()\n",
    "    ax4.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"pareto_frontiers.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualizations saved: {OUTPUT_DIR / 'pareto_frontiers.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6791a",
   "metadata": {},
   "source": [
    "## Cell 17: Summary Statistics Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_stats = (\n",
    "    results_df.groupby([\"precision\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"mean_latency_s\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"mean_peak_mem_mb\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"tokens_per_second\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "        }\n",
    "    )\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "if \"energy_kwh\" in results_df.columns:\n",
    "    energy_stats = (\n",
    "        results_df.groupby([\"precision\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"energy_kwh\": [\"mean\", \"std\"],\n",
    "                \"avg_power_watts\": [\"mean\", \"std\"],\n",
    "            }\n",
    "        )\n",
    "        .round(4)\n",
    "    )\n",
    "    summary_stats = pd.concat([summary_stats, energy_stats], axis=1)\n",
    "\n",
    "print(\"\\n{'#'*70}\")\n",
    "print(\"SUMMARY STATISTICS BY PRECISION\")\n",
    "print(f\"{'#' * 70}\\n\")\n",
    "print(summary_stats)\n",
    "\n",
    "summary_stats.to_csv(OUTPUT_DIR / \"summary_statistics.csv\")\n",
    "print(f\"\\n✓ Summary saved: {OUTPUT_DIR / 'summary_statistics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d160a5",
   "metadata": {},
   "source": [
    "## Cell 18: Paper-Ready Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a192ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create formatted table for academic papers\n",
    "paper_columns = [\n",
    "    \"model\",\n",
    "    \"precision\",\n",
    "    \"mean_latency_s\",\n",
    "    \"mean_peak_mem_mb\",\n",
    "    \"tokens_per_second\",\n",
    "]\n",
    "\n",
    "if \"energy_kwh\" in results_df.columns:\n",
    "    paper_columns.append(\"energy_kwh\")\n",
    "\n",
    "accuracy_cols = [col for col in results_df.columns if \"accuracy\" in col]\n",
    "if accuracy_cols:\n",
    "    paper_columns.extend(accuracy_cols[:3])  # Include first 3 accuracy metrics\n",
    "\n",
    "paper_table = results_df[paper_columns].copy()\n",
    "paper_table[\"model\"] = paper_table[\"model\"].str.split(\"/\").str[-1]\n",
    "\n",
    "# Rename columns for readability\n",
    "rename_map = {\n",
    "    \"mean_latency_s\": \"Latency (s)\",\n",
    "    \"mean_peak_mem_mb\": \"Memory (MB)\",\n",
    "    \"tokens_per_second\": \"Tokens/s\",\n",
    "    \"energy_kwh\": \"Energy (kWh)\",\n",
    "}\n",
    "paper_table = paper_table.rename(columns=rename_map)\n",
    "paper_table = paper_table.round(4)\n",
    "\n",
    "print(\"\\n{'#'*70}\")\n",
    "print(\"PAPER-READY RESULTS TABLE\")\n",
    "print(f\"{'#' * 70}\\n\")\n",
    "print(paper_table.to_string(index=False))\n",
    "\n",
    "# Save as CSV and LaTeX\n",
    "paper_table.to_csv(OUTPUT_DIR / \"paper_results.csv\", index=False)\n",
    "latex_table = paper_table.to_latex(index=False, float_format=\"%.4f\")\n",
    "with open(OUTPUT_DIR / \"paper_results.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"\\n✓ Paper table saved: {OUTPUT_DIR / 'paper_results.csv'}\")\n",
    "print(f\"✓ LaTeX table saved: {OUTPUT_DIR / 'paper_results.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a73ce0",
   "metadata": {},
   "source": [
    "## Cell 19: Export JSON Results (for sharing/archiving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ea11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all results as JSON for easy sharing\n",
    "json_output = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"models\": MODELS,\n",
    "        \"precisions\": PRECISIONS,\n",
    "        \"accuracy_tasks\": ACCURACY_TASKS,\n",
    "        \"num_runs\": NUM_RUNS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "    },\n",
    "    \"results\": results_df.to_dict(orient=\"records\"),\n",
    "}\n",
    "\n",
    "if analysis_results:\n",
    "    json_output[\"analysis\"] = analysis_df.to_dict(orient=\"records\")\n",
    "\n",
    "json_path = OUTPUT_DIR / \"complete_results.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(json_output, f, indent=2)\n",
    "\n",
    "print(f\"✓ JSON results saved: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d2bca",
   "metadata": {},
   "source": [
    "## Cell 20: Generate Executive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text report summarizing key findings\n",
    "report_lines = [\n",
    "    \"=\" * 70,\n",
    "    \"SLiM-EVAL: EXECUTIVE SUMMARY REPORT\",\n",
    "    \"=\" * 70,\n",
    "    f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    f\"\\nModels Evaluated: {len(results_df['model'].unique())}\",\n",
    "    f\"Precision Modes: {', '.join(results_df['precision'].unique())}\",\n",
    "    f\"Total Configurations: {len(results_df)}\",\n",
    "    \"\\n\" + \"=\" * 70,\n",
    "    \"\\nKEY FINDINGS:\",\n",
    "    \"=\" * 70,\n",
    "]\n",
    "\n",
    "# Find best performers\n",
    "if len(results_df) > 0:\n",
    "    # Fastest model\n",
    "    fastest = results_df.loc[results_df[\"mean_latency_s\"].idxmin()]\n",
    "    report_lines.append(f\"\\n1. FASTEST MODEL:\")\n",
    "    report_lines.append(\n",
    "        f\"   {fastest['model'].split('/')[-1]} ({fastest['precision']})\"\n",
    "    )\n",
    "    report_lines.append(f\"   Latency: {fastest['mean_latency_s']:.4f}s\")\n",
    "\n",
    "    # Most memory efficient\n",
    "    mem_efficient = results_df.loc[results_df[\"mean_peak_mem_mb\"].idxmin()]\n",
    "    report_lines.append(f\"\\n2. MOST MEMORY EFFICIENT:\")\n",
    "    report_lines.append(\n",
    "        f\"   {mem_efficient['model'].split('/')[-1]} ({mem_efficient['precision']})\"\n",
    "    )\n",
    "    report_lines.append(f\"   Memory: {mem_efficient['mean_peak_mem_mb']:.2f} MB\")\n",
    "\n",
    "    # Highest throughput\n",
    "    highest_throughput = results_df.loc[results_df[\"tokens_per_second\"].idxmax()]\n",
    "    report_lines.append(f\"\\n3. HIGHEST THROUGHPUT:\")\n",
    "    report_lines.append(\n",
    "        f\"   {highest_throughput['model'].split('/')[-1]} ({highest_throughput['precision']})\"\n",
    "    )\n",
    "    report_lines.append(\n",
    "        f\"   Throughput: {highest_throughput['tokens_per_second']:.2f} tokens/s\"\n",
    "    )\n",
    "\n",
    "    # Best accuracy (if available)\n",
    "    accuracy_cols = [col for col in results_df.columns if \"accuracy\" in col]\n",
    "    if accuracy_cols:\n",
    "        results_df[\"avg_accuracy\"] = results_df[accuracy_cols].mean(axis=1)\n",
    "        best_accuracy = results_df.loc[results_df[\"avg_accuracy\"].idxmax()]\n",
    "        report_lines.append(f\"\\n4. BEST AVERAGE ACCURACY:\")\n",
    "        report_lines.append(\n",
    "            f\"   {best_accuracy['model'].split('/')[-1]} ({best_accuracy['precision']})\"\n",
    "        )\n",
    "        report_lines.append(f\"   Avg Accuracy: {best_accuracy['avg_accuracy']:.4f}\")\n",
    "\n",
    "    # Quantization impact\n",
    "    if analysis_results:\n",
    "        report_lines.append(f\"\\n\" + \"=\" * 70)\n",
    "        report_lines.append(\"QUANTIZATION IMPACT:\")\n",
    "        report_lines.append(\"=\" * 70)\n",
    "\n",
    "        avg_speedup = analysis_df[\"speedup\"].mean()\n",
    "        avg_mem_reduction = analysis_df[\"memory_reduction_pct\"].mean()\n",
    "\n",
    "        report_lines.append(f\"\\nINT8/INT4 Quantization Effects (Average):\")\n",
    "        report_lines.append(f\"  • Speedup: {avg_speedup:.2f}x\")\n",
    "        report_lines.append(f\"  • Memory Reduction: {avg_mem_reduction:.1f}%\")\n",
    "\n",
    "        if \"energy_reduction_pct\" in analysis_df.columns:\n",
    "            avg_energy_reduction = analysis_df[\"energy_reduction_pct\"].mean()\n",
    "            report_lines.append(f\"  • Energy Reduction: {avg_energy_reduction:.1f}%\")\n",
    "\n",
    "        acc_drop_cols = [col for col in analysis_df.columns if \"acc_drop\" in col]\n",
    "        if acc_drop_cols:\n",
    "            avg_acc_drop = analysis_df[acc_drop_cols].mean().mean()\n",
    "            report_lines.append(f\"  • Average Accuracy Drop: {avg_acc_drop:.2f}%\")\n",
    "\n",
    "report_lines.append(\"\\n\" + \"=\" * 70)\n",
    "report_lines.append(\"END OF REPORT\")\n",
    "report_lines.append(\"=\" * 70)\n",
    "\n",
    "report_text = \"\\n\".join(report_lines)\n",
    "print(report_text)\n",
    "\n",
    "# Save report\n",
    "report_path = OUTPUT_DIR / \"executive_summary.txt\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(f\"\\n✓ Executive summary saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe55870",
   "metadata": {},
   "source": [
    "## Cell 21: Quick Single Model Test (For Debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ee03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test a single model quickly\n",
    "# TEST_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# TEST_PRECISION = \"fp16\"\n",
    "#\n",
    "# print(f\"Quick test: {TEST_MODEL} in {TEST_PRECISION}\")\n",
    "# results = run_complete_benchmark(TEST_MODEL, TEST_PRECISION)\n",
    "# if results:\n",
    "#     print(\"\\nTest Results:\")\n",
    "#     for key, value in results.items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"Quick test cell ready (not executed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997f313",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary of Complete Framework\n",
    "\n",
    "### Metrics Tracked:\n",
    "1. **Latency** - Mean, median, P95, P99 inference time\n",
    "2. **Memory** - Peak and average GPU memory usage\n",
    "3. **Energy** - Power consumption and energy per query\n",
    "4. **Accuracy** - Task performance on MMLU, GSM8K, HellaSwag, etc.\n",
    "\n",
    "### Workflow:\n",
    "1. **Setup** - Configure models, precisions, benchmarks\n",
    "2. **Quantize** (optional) - Pre-quantize models with llm-compressor\n",
    "3. **Benchmark** - Run latency, memory, energy, and accuracy tests\n",
    "4. **Analyze** - Calculate trade-offs and efficiency metrics\n",
    "5. **Visualize** - Generate plots and Pareto frontiers\n",
    "6. **Report** - Export results in CSV, JSON, LaTeX formats\n",
    "\n",
    "### Next Steps:\n",
    "- Add carbon emissions tracking (multiply energy by grid carbon intensity)\n",
    "- Integrate additional benchmarks (BBH, MATH, MedQA, LegalBench)\n",
    "- Add throughput benchmarking under different batch sizes\n",
    "- Implement automated Pareto frontier analysis\n",
    "- Add statistical significance testing for accuracy differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slim-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
