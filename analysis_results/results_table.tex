\begin{tabular}{lrlrrrrrrrr}
\toprule
model & Params (B) & precision & Size (GB) & Latency (s) & Memory (MB) & Tokens/s & Energy (kWh) & mmlu_accuracy & gsm8k_accuracy & hellaswag_accuracy \\
\midrule
Mistral-7B-Instruct-v0.3 & 6.7109 & fp16 & 13.5005 & 0.1833 & 74882.6250 & 714.6926 & 0.0384 & 0.6184 & 0.4996 & 0.6591 \\
Mistral-7B-Instruct-v0.3 & 6.7109 & int8 & 7.5062 & 0.1264 & 74882.6250 & 989.1840 & 0.0211 & 0.6174 & 0.4708 & 0.6567 \\
Mistral-7B-Instruct-v0.3 & 6.7109 & int4 & 4.4542 & 0.1127 & 74954.6250 & 1420.5349 & 0.0144 & 0.6062 & 0.4549 & 0.6544 \\
Phi-3-mini-4k-instruct & 3.8209 & int8 & 4.1134 & 0.1796 & 75166.6250 & 1425.0647 & 0.0116 & 0.6953 & 0.7255 & 0.5955 \\
Phi-3-mini-4k-instruct & 3.8209 & fp16 & 7.1173 & 0.2378 & 75126.6250 & 1076.7286 & 0.0214 & 0.7051 & 0.7976 & 0.5995 \\
Phi-3-mini-4k-instruct & 3.8209 & int4 & 2.5276 & 0.2361 & 75164.6250 & 1084.2027 & 0.0106 & 0.6828 & 0.7180 & 0.5845 \\
Llama-3.2-3B-Instruct & 3.9589 & int8 & 5.5641 & 0.0989 & 75490.6250 & 1517.0678 & 0.0136 & 0.6051 & 0.6717 & 0.5282 \\
Llama-3.2-3B-Instruct & 3.9589 & fp16 & 5.9842 & 0.2152 & 75422.6250 & 1189.2299 & 0.0210 & 0.6054 & 0.6778 & 0.5278 \\
Llama-3.2-3B-Instruct & 3.9589 & int4 & 4.3308 & 0.1322 & 75448.6250 & 1936.1319 & 0.0107 & 0.5879 & 0.6012 & 0.5195 \\
Qwen2.5-3B-Instruct & 2.4343 & int4 & 1.1335 & 0.1527 & 75618.6250 & 1583.0411 & 0.0113 & 0.6416 & 0.5353 & 0.5493 \\
Qwen2.5-3B-Instruct & 2.4343 & fp16 & 5.7481 & 0.1458 & 75630.6250 & 1186.2833 & 0.0174 & 0.6635 & 0.6566 & 0.5604 \\
Qwen2.5-3B-Instruct & 2.4343 & int8 & 4.9072 & 0.1478 & 74973.6250 & 811.9352 & 0.0152 & 0.6555 & 0.6490 & 0.5518 \\
gemma-3-4b-it & 4.3000 & fp16 & 8.6400 & 0.2598 & 74380.6250 & 966.1879 & 0.0267 & 0.5835 & 0.7635 & 0.5595 \\
\bottomrule
\end{tabular}
